Bootstrap: docker
From: nvidia/cuda:12.8.0-devel-ubuntu22.04

%environment
    export PATH=/opt/conda/bin:$PATH
    export CUDA_HOME=/usr/local/cuda
    export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH

%post
    # Update and install basic dependencies
    apt-get update && apt-get install -y \
        wget \
        git \
        build-essential \
        curl \
        ca-certificates \
        && update-ca-certificates \
        && rm -rf /var/lib/apt/lists/*

    # Install Miniconda
    wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O /tmp/miniconda.sh
    bash /tmp/miniconda.sh -b -p /opt/conda
    rm /tmp/miniconda.sh
    export PATH=/opt/conda/bin:$PATH

    # Accept conda TOS and configure to use conda-forge
    conda config --set channel_priority flexible
    conda config --add channels conda-forge
    yes | conda tos accept || true

    # Create conda environment and install vLLM
    conda create -n vllm python=3.11 -y -c conda-forge
    . /opt/conda/etc/profile.d/conda.sh
    conda activate vllm

    # Install PyTorch with explicit CUDA 12.8 support
    pip install --no-cache-dir torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128
    
    # Install latest vLLM with CUDA support (supports Qwen3)
    pip install --no-cache-dir vllm --upgrade
    pip install --no-cache-dir huggingface-hub

    # Pre-download the model (optional, can be done at runtime)
    # Uncomment if you want to bake the model into the container
    # python -c "from huggingface_hub import snapshot_download; snapshot_download('Qwen/Qwen3-30B-A3B-Instruct-2507')"

    # Clean up
    conda clean -a -y
    apt-get clean

%runscript
    #!/bin/bash
    . /opt/conda/etc/profile.d/conda.sh
    conda activate vllm
    
    # Force offline mode - work with cached model only
    export HF_HUB_OFFLINE=1
    export TRANSFORMERS_OFFLINE=1
    
    # Fix SSL certificate issues from host environment
    # If SSL_CERT_FILE is set but doesn't exist, unset it
    if [ -n "${SSL_CERT_FILE}" ] && [ ! -f "${SSL_CERT_FILE}" ]; then
        echo "Warning: SSL_CERT_FILE points to non-existent file: ${SSL_CERT_FILE}"
        echo "Unsetting SSL_CERT_FILE to use system certificates"
        unset SSL_CERT_FILE
    fi
    
    # Default values
    PORT=${PORT:-8000}
    # Use local snapshot path instead of repo name to avoid HF lookups
    MODEL=${MODEL:-"/root/.cache/huggingface/models--Qwen--Qwen3-30B-A3B-Instruct-2507/snapshots/0d7cf23991f47feeb3a57ecb4c9cee8ea4a17bfe"}
    TENSOR_PARALLEL=${TENSOR_PARALLEL:-1}
    GPU_MEMORY=${GPU_MEMORY:-0.95}
    MAX_MODEL_LEN=${MAX_MODEL_LEN:-32768}
    
    echo "Starting vLLM server for $MODEL on port $PORT"
    echo "Tensor parallel size: $TENSOR_PARALLEL"
    echo "GPU memory utilization: $GPU_MEMORY"
    echo "Max model length: $MAX_MODEL_LEN"
    
    python -m vllm.entrypoints.openai.api_server \
        --model "$MODEL" \
        --port "$PORT" \
        --tensor-parallel-size "$TENSOR_PARALLEL" \
        --gpu-memory-utilization "$GPU_MEMORY" \
        --max-model-len "$MAX_MODEL_LEN" \
        --download-dir /root/.cache/huggingface \
        --trust-remote-code \
        --served-model-name "qwen3-30b"

%labels
    Author rm4411
    Version v1.0
    Description vLLM server for Qwen3-30B-A3B-Instruct-2507

%help
    This container runs a vLLM server for the Qwen3-30B-A3B-Instruct-2507 model.
    
    Usage:
        apptainer run --nv qwen3-30b-vllm.sif
    
    Environment variables:
        PORT            - API port (default: 8000)
        MODEL           - Model name (default: Qwen/Qwen3-30B-A3B-Instruct-2507)
        TENSOR_PARALLEL - Number of GPUs for tensor parallelism (default: 1)
        GPU_MEMORY      - GPU memory utilization fraction (default: 0.95)
        MAX_MODEL_LEN   - Maximum model context length (default: 32768)
        HF_TOKEN        - Hugging Face token for private models
